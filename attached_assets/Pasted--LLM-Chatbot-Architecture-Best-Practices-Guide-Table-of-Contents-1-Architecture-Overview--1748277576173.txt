# LLM Chatbot Architecture Best Practices Guide

## Table of Contents
1. [Architecture Overview](#architecture-overview)
2. [Backend Architecture](#backend-architecture)
3. [Frontend Architecture](#frontend-architecture)
4. [State Management](#state-management)
5. [Performance Optimization](#performance-optimization)
6. [Security & Safety](#security-safety)
7. [Scalability Patterns](#scalability-patterns)
8. [Testing & Monitoring](#testing-monitoring)
9. [Implementation Checklist](#implementation-checklist)

## 1. Architecture Overview

### Recommended Tech Stack
```yaml
Frontend:
  - React 18+ with TypeScript
  - React Query (TanStack Query) for server state
  - WebSocket/SSE for streaming responses
  - Vite for build tooling
  - Tailwind CSS for styling

Backend:
  - Node.js/Express or Python/FastAPI
  - Redis for caching and session management
  - PostgreSQL for conversation history
  - Queue system (Bull/Celery) for async processing
  - WebSocket server for real-time communication

Infrastructure:
  - Docker containers
  - Kubernetes for orchestration
  - CloudFlare for CDN and DDoS protection
  - S3/GCS for file storage
  - Prometheus + Grafana for monitoring
```

### High-Level Architecture
```mermaid
graph TB
    Client[React Client] -->|WebSocket/HTTPS| Gateway[API Gateway]
    Gateway --> LB[Load Balancer]
    LB --> API1[API Server 1]
    LB --> API2[API Server 2]
    API1 --> Queue[Message Queue]
    API2 --> Queue
    Queue --> Worker1[LLM Worker 1]
    Queue --> Worker2[LLM Worker 2]
    API1 --> Cache[(Redis Cache)]
    API2 --> Cache
    API1 --> DB[(PostgreSQL)]
    API2 --> DB
    Worker1 --> LLM[LLM API]
    Worker2 --> LLM
    Worker1 --> Cache
    Worker2 --> Cache
```

## 2. Backend Architecture

### API Design Patterns

#### 2.1 Streaming Response Handler
```typescript
// server/handlers/chat.ts
import { EventEmitter } from 'events';

class StreamingChatHandler {
  private responseEmitter = new EventEmitter();
  
  async handleChatStream(
    conversationId: string,
    message: string,
    context: ChatContext
  ): Promise<ReadableStream> {
    // 1. Validate input
    const validated = await this.validateInput(message);
    
    // 2. Check rate limits
    await this.checkRateLimit(context.userId);
    
    // 3. Load conversation history
    const history = await this.loadHistory(conversationId);
    
    // 4. Prepare prompt with context
    const prompt = this.buildPrompt(message, history, context);
    
    // 5. Stream from LLM
    const stream = new ReadableStream({
      async start(controller) {
        try {
          const llmStream = await llmClient.createChatStream({
            messages: prompt,
            temperature: context.temperature || 0.7,
            max_tokens: context.maxTokens || 2000,
          });
          
          for await (const chunk of llmStream) {
            // Process chunk
            const processed = await this.processChunk(chunk);
            controller.enqueue(processed);
            
            // Cache partial response
            await this.cachePartial(conversationId, processed);
          }
          
          controller.close();
        } catch (error) {
          controller.error(error);
        }
      }
    });
    
    return stream;
  }
  
  private buildPrompt(
    message: string,
    history: Message[],
    context: ChatContext
  ): ChatMessage[] {
    const systemPrompt = this.generateSystemPrompt(context);
    const contextWindow = this.selectRelevantHistory(history);
    
    return [
      { role: 'system', content: systemPrompt },
      ...contextWindow,
      { role: 'user', content: message }
    ];
  }
}
```

#### 2.2 Conversation Management
```typescript
// server/services/conversationService.ts
interface ConversationService {
  // Efficient history management
  async getConversation(id: string): Promise<Conversation> {
    // Try cache first
    const cached = await redis.get(`conv:${id}`);
    if (cached) return JSON.parse(cached);
    
    // Load from DB with pagination
    const conversation = await db.conversation.findUnique({
      where: { id },
      include: {
        messages: {
          orderBy: { createdAt: 'desc' },
          take: 50 // Limit initial load
        }
      }
    });
    
    // Cache with TTL
    await redis.setex(`conv:${id}`, 3600, JSON.stringify(conversation));
    return conversation;
  }
  
  // Message queueing for reliability
  async queueMessage(message: ChatMessage): Promise<void> {
    await messageQueue.add('process-message', {
      ...message,
      timestamp: Date.now(),
      retries: 0
    }, {
      attempts: 3,
      backoff: {
        type: 'exponential',
        delay: 2000
      }
    });
  }
  
  // Context window management
  async getContextWindow(
    conversationId: string,
    maxTokens: number = 4000
  ): Promise<Message[]> {
    const messages = await this.getRecentMessages(conversationId);
    return this.truncateToTokenLimit(messages, maxTokens);
  }
}
```

#### 2.3 Error Handling & Fallbacks
```typescript
// server/middleware/errorHandler.ts
class ChatErrorHandler {
  async handleLLMError(error: any, context: ErrorContext) {
    // Log error with context
    logger.error('LLM Error', { error, context });
    
    // Determine error type and response
    if (error.code === 'rate_limit_exceeded') {
      return this.handleRateLimit(context);
    } else if (error.code === 'context_length_exceeded') {
      return this.handleContextOverflow(context);
    } else if (error.code === 'timeout') {
      return this.handleTimeout(context);
    }
    
    // Fallback response
    return {
      error: true,
      message: "I'm having trouble processing your request. Please try again.",
      fallbackSuggestions: this.generateFallbackSuggestions(context)
    };
  }
  
  private async handleRateLimit(context: ErrorContext) {
    // Queue for later processing
    await this.queueForRetry(context);
    
    return {
      error: true,
      message: "I'm a bit busy right now. Your message has been queued.",
      retryAfter: this.calculateRetryTime()
    };
  }
}
```

### Database Schema
```sql
-- Optimized schema for chat history
CREATE TABLE conversations (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL REFERENCES users(id),
  title TEXT,
  model_config JSONB DEFAULT '{}',
  metadata JSONB DEFAULT '{}',
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW(),
  last_message_at TIMESTAMPTZ,
  message_count INTEGER DEFAULT 0,
  total_tokens INTEGER DEFAULT 0,
  
  -- Indexes for performance
  INDEX idx_user_conversations ON conversations(user_id, updated_at DESC),
  INDEX idx_active_conversations ON conversations(last_message_at DESC)
);

CREATE TABLE messages (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  conversation_id UUID NOT NULL REFERENCES conversations(id) ON DELETE CASCADE,
  role TEXT NOT NULL CHECK (role IN ('user', 'assistant', 'system')),
  content TEXT NOT NULL,
  tokens INTEGER,
  metadata JSONB DEFAULT '{}',
  created_at TIMESTAMPTZ DEFAULT NOW(),
  
  -- For efficient pagination
  sequence_number SERIAL,
  
  -- Indexes
  INDEX idx_conversation_messages ON messages(conversation_id, sequence_number DESC),
  INDEX idx_message_search ON messages USING gin(to_tsvector('english', content))
);

-- Partitioning for scale
CREATE TABLE messages_2024_01 PARTITION OF messages
  FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');
```

## 3. Frontend Architecture

### 3.1 React Component Structure
```typescript
// components/Chat/ChatInterface.tsx
const ChatInterface: React.FC = () => {
  const { conversationId } = useConversation();
  const { messages, isLoading, streamingMessage } = useChatMessages(conversationId);
  const { sendMessage, cancelStream } = useChatActions();
  
  return (
    <div className="flex flex-col h-full">
      <ChatHeader />
      <MessageList 
        messages={messages}
        streamingMessage={streamingMessage}
        onRetry={sendMessage}
      />
      <ChatInput 
        onSend={sendMessage}
        onCancel={cancelStream}
        isStreaming={!!streamingMessage}
      />
    </div>
  );
};

// Virtualized message list for performance
const MessageList: React.FC<MessageListProps> = React.memo(({ 
  messages, 
  streamingMessage 
}) => {
  const listRef = useRef<VariableSizeList>(null);
  const rowHeights = useRef<{ [key: string]: number }>({});
  
  // Auto-scroll to bottom on new messages
  useEffect(() => {
    if (listRef.current && messages.length > 0) {
      listRef.current.scrollToItem(messages.length - 1, 'end');
    }
  }, [messages.length]);
  
  const getRowHeight = useCallback((index: number) => {
    return rowHeights.current[index] || 100;
  }, []);
  
  const Row = useCallback(({ index, style }) => {
    const message = index === messages.length && streamingMessage 
      ? streamingMessage 
      : messages[index];
      
    return (
      <div style={style}>
        <MessageBubble 
          message={message}
          onHeightChange={(height) => {
            rowHeights.current[index] = height;
            listRef.current?.resetAfterIndex(index);
          }}
        />
      </div>
    );
  }, [messages, streamingMessage]);
  
  return (
    <VariableSizeList
      ref={listRef}
      height={600}
      itemCount={messages.length + (streamingMessage ? 1 : 0)}
      itemSize={getRowHeight}
      width="100%"
    >
      {Row}
    </VariableSizeList>
  );
});
```

### 3.2 WebSocket/SSE Management
```typescript
// hooks/useStreamingChat.ts
const useStreamingChat = (conversationId: string) => {
  const [streamingMessage, setStreamingMessage] = useState<Message | null>(null);
  const abortControllerRef = useRef<AbortController | null>(null);
  
  const sendMessage = useCallback(async (content: string) => {
    // Cancel any existing stream
    abortControllerRef.current?.abort();
    abortControllerRef.current = new AbortController();
    
    try {
      const response = await fetch('/api/chat/stream', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Accept': 'text/event-stream',
        },
        body: JSON.stringify({ conversationId, content }),
        signal: abortControllerRef.current.signal,
      });
      
      const reader = response.body?.getReader();
      const decoder = new TextDecoder();
      
      let accumulatedContent = '';
      
      while (reader) {
        const { done, value } = await reader.read();
        if (done) break;
        
        const chunk = decoder.decode(value);
        const lines = chunk.split('\n');
        
        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = JSON.parse(line.slice(6));
            
            if (data.type === 'token') {
              accumulatedContent += data.content;
              setStreamingMessage({
                role: 'assistant',
                content: accumulatedContent,
                isStreaming: true
              });
            } else if (data.type === 'done') {
              setStreamingMessage(null);
              // Add to messages
              addMessage({
                role: 'assistant',
                content: accumulatedContent,
                metadata: data.metadata
              });
            }
          }
        }
      }
    } catch (error) {
      if (error.name !== 'AbortError') {
        handleError(error);
      }
    }
  }, [conversationId]);
  
  return { sendMessage, streamingMessage };
};
```

### 3.3 Optimistic Updates & Error Recovery
```typescript
// hooks/useOptimisticChat.ts
const useOptimisticChat = () => {
  const queryClient = useQueryClient();
  
  const sendMessage = useMutation({
    mutationFn: async (message: string) => {
      return chatAPI.sendMessage(message);
    },
    onMutate: async (message) => {
      // Cancel outgoing refetches
      await queryClient.cancelQueries(['messages']);
      
      // Snapshot previous value
      const previousMessages = queryClient.getQueryData(['messages']);
      
      // Optimistically update
      queryClient.setQueryData(['messages'], (old: Message[]) => [
        ...old,
        {
          id: `temp-${Date.now()}`,
          role: 'user',
          content: message,
          timestamp: new Date().toISOString(),
          status: 'sending'
        }
      ]);
      
      return { previousMessages };
    },
    onError: (err, message, context) => {
      // Rollback on error
      queryClient.setQueryData(['messages'], context.previousMessages);
      
      // Show error state
      toast.error('Failed to send message. Retrying...');
      
      // Retry logic
      setTimeout(() => {
        sendMessage.mutate(message);
      }, 2000);
    },
    onSettled: () => {
      queryClient.invalidateQueries(['messages']);
    }
  });
  
  return sendMessage;
};
```

## 4. State Management

### 4.1 Conversation State Architecture
```typescript
// store/conversationStore.ts
interface ConversationStore {
  // Current state
  activeConversationId: string | null;
  conversations: Map<string, Conversation>;
  messages: Map<string, Message[]>;
  
  // UI state
  streamingMessages: Map<string, StreamingMessage>;
  typingIndicators: Set<string>;
  
  // Actions
  loadConversation: (id: string) => Promise<void>;
  sendMessage: (conversationId: string, content: string) => Promise<void>;
  retryMessage: (messageId: string) => Promise<void>;
  
  // Optimizations
  prefetchConversations: () => Promise<void>;
  cleanupOldMessages: () => void;
}

// Efficient message storage with pagination
class MessageStore {
  private messages = new Map<string, Message[]>();
  private hasMore = new Map<string, boolean>();
  private cursors = new Map<string, string>();
  
  async loadMessages(conversationId: string, cursor?: string) {
    const response = await api.getMessages(conversationId, {
      cursor,
      limit: 50
    });
    
    const existing = this.messages.get(conversationId) || [];
    this.messages.set(conversationId, [...response.messages, ...existing]);
    this.hasMore.set(conversationId, response.hasMore);
    this.cursors.set(conversationId, response.nextCursor);
  }
  
  getMessages(conversationId: string): Message[] {
    return this.messages.get(conversationId) || [];
  }
}
```

### 4.2 React Query Configuration
```typescript
// lib/queryClient.ts
export const queryClient = new QueryClient({
  defaultOptions: {
    queries: {
      // Stale time for chat data
      staleTime: 5 * 60 * 1000, // 5 minutes
      cacheTime: 10 * 60 * 1000, // 10 minutes
      
      // Retry configuration
      retry: (failureCount, error) => {
        if (error.status === 429) return failureCount < 3;
        if (error.status >= 500) return failureCount < 2;
        return false;
      },
      
      // Background refetch
      refetchOnWindowFocus: false,
      refetchOnReconnect: 'always',
    },
    mutations: {
      // Optimistic updates by default
      onError: (error) => {
        console.error('Mutation error:', error);
      },
    },
  },
});

// Prefetch conversations on app load
queryClient.prefetchQuery({
  queryKey: ['conversations'],
  queryFn: fetchRecentConversations,
  staleTime: Infinity,
});
```

## 5. Performance Optimization

### 5.1 Message Rendering Optimization
```typescript
// components/MessageBubble.tsx
const MessageBubble = React.memo(({ message, onHeightChange }) => {
  const [isExpanded, setIsExpanded] = useState(false);
  const contentRef = useRef<HTMLDivElement>(null);
  
  // Measure height for virtualization
  useEffect(() => {
    if (contentRef.current) {
      const height = contentRef.current.offsetHeight;
      onHeightChange?.(height);
    }
  }, [message.content, isExpanded]);
  
  // Lazy render markdown
  const renderedContent = useMemo(() => {
    if (message.content.length < 500) {
      return <MarkdownRenderer content={message.content} />;
    }
    
    // For long messages, render preview
    return (
      <>
        <MarkdownRenderer 
          content={isExpanded ? message.content : message.content.slice(0, 500) + '...'} 
        />
        {!isExpanded && (
          <button onClick={() => setIsExpanded(true)}>
            Show more
          </button>
        )}
      </>
    );
  }, [message.content, isExpanded]);
  
  return (
    <div ref={contentRef} className="message-bubble">
      {renderedContent}
    </div>
  );
}, (prevProps, nextProps) => {
  // Custom comparison for memo
  return prevProps.message.id === nextProps.message.id &&
         prevProps.message.content === nextProps.message.content;
});
```

### 5.2 Network Optimization
```typescript
// services/chatService.ts
class ChatService {
  private pendingRequests = new Map<string, Promise<any>>();
  
  // Request deduplication
  async sendMessage(conversationId: string, content: string) {
    const key = `${conversationId}:${content}`;
    
    if (this.pendingRequests.has(key)) {
      return this.pendingRequests.get(key);
    }
    
    const promise = this._sendMessage(conversationId, content);
    this.pendingRequests.set(key, promise);
    
    try {
      const result = await promise;
      return result;
    } finally {
      this.pendingRequests.delete(key);
    }
  }
  
  // Batch multiple requests
  private messageQueue: Array<{ conversationId: string; content: string }> = [];
  private batchTimer: NodeJS.Timeout | null = null;
  
  async queueMessage(conversationId: string, content: string) {
    this.messageQueue.push({ conversationId, content });
    
    if (!this.batchTimer) {
      this.batchTimer = setTimeout(() => this.flushMessageQueue(), 100);
    }
  }
  
  private async flushMessageQueue() {
    const messages = [...this.messageQueue];
    this.messageQueue = [];
    this.batchTimer = null;
    
    if (messages.length === 0) return;
    
    // Send batch request
    await api.post('/chat/batch', { messages });
  }
}
```

### 5.3 Caching Strategy
```typescript
// Cache layers for optimal performance
const cacheStrategy = {
  // L1: In-memory cache for active conversations
  memory: new Map<string, CachedConversation>(),
  
  // L2: IndexedDB for offline support
  async getFromIndexedDB(id: string) {
    const db = await openDB('chatCache', 1);
    return db.get('conversations', id);
  },
  
  // L3: Redis for shared cache
  async getFromRedis(id: string) {
    return redis.get(`conv:${id}`);
  },
  
  // Waterfall through cache layers
  async get(id: string): Promise<Conversation | null> {
    // Check memory
    if (this.memory.has(id)) {
      return this.memory.get(id)!;
    }
    
    // Check IndexedDB
    const cached = await this.getFromIndexedDB(id);
    if (cached) {
      this.memory.set(id, cached);
      return cached;
    }
    
    // Check Redis
    const remote = await this.getFromRedis(id);
    if (remote) {
      this.memory.set(id, remote);
      await this.saveToIndexedDB(id, remote);
      return remote;
    }
    
    return null;
  }
};
```

## 6. Security & Safety

### 6.1 Input Validation & Sanitization
```typescript
// security/inputValidation.ts
class ChatInputValidator {
  private readonly MAX_MESSAGE_LENGTH = 4000;
  private readonly RATE_LIMIT_WINDOW = 60000; // 1 minute
  private readonly MAX_MESSAGES_PER_WINDOW = 20;
  
  async validateMessage(
    message: string,
    userId: string
  ): Promise<ValidationResult> {
    // Length check
    if (message.length > this.MAX_MESSAGE_LENGTH) {
      return { valid: false, error: 'Message too long' };
    }
    
    // Content filtering
    const filtered = await this.filterContent(message);
    if (filtered.blocked) {
      return { valid: false, error: 'Content policy violation' };
    }
    
    // Rate limiting
    const rateLimitOk = await this.checkRateLimit(userId);
    if (!rateLimitOk) {
      return { valid: false, error: 'Rate limit exceeded' };
    }
    
    // Injection prevention
    const sanitized = this.sanitizeInput(message);
    
    return { valid: true, sanitized };
  }
  
  private sanitizeInput(input: string): string {
    // Remove potential prompt injections
    const cleaned = input
      .replace(/\[INST\]|\[\/INST\]/g, '')
      .replace(/\<\|im_start\|\>|\<\|im_end\|\>/g, '')
      .replace(/^(system|assistant):/gmi, '');
    
    return cleaned.trim();
  }
  
  private async filterContent(message: string): Promise<FilterResult> {
    // Use moderation API
    const moderation = await openai.createModeration({ input: message });
    
    if (moderation.data.results[0].flagged) {
      return {
        blocked: true,
        categories: moderation.data.results[0].categories
      };
    }
    
    return { blocked: false };
  }
}
```

### 6.2 Authentication & Authorization
```typescript
// middleware/auth.ts
export const authenticateWebSocket = async (ws: WebSocket, req: Request) => {
  const token = req.headers.authorization?.replace('Bearer ', '');
  
  if (!token) {
    ws.close(1008, 'Unauthorized');
    return null;
  }
  
  try {
    const decoded = jwt.verify(token, process.env.JWT_SECRET!);
    const user = await getUserById(decoded.userId);
    
    if (!user || !user.canUseChat) {
      ws.close(1008, 'Forbidden');
      return null;
    }
    
    // Attach user to websocket
    ws.userId = user.id;
    ws.rateLimiter = new RateLimiter(user.tier);
    
    return user;
  } catch (error) {
    ws.close(1008, 'Invalid token');
    return null;
  }
};
```

## 7. Scalability Patterns

### 7.1 Horizontal Scaling Architecture
```yaml
# docker-compose.yml for development
version: '3.8'

services:
  api:
    build: ./api
    environment:
      - NODE_ENV=production
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql://postgres:password@db:5432/chat
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
    depends_on:
      - redis
      - db
      
  worker:
    build: ./worker
    environment:
      - WORKER_CONCURRENCY=4
      - QUEUE_URL=redis://redis:6379
    deploy:
      replicas: 2
      
  nginx:
    image: nginx:alpine
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    ports:
      - "80:80"
    depends_on:
      - api
```

### 7.2 Queue-Based Architecture
```typescript
// workers/chatWorker.ts
class ChatWorker {
  constructor(
    private queue: Queue,
    private llmPool: LLMConnectionPool
  ) {
    this.queue.process('chat-completion', this.concurrency, this.processMessage.bind(this));
  }
  
  async processMessage(job: Job<ChatJob>) {
    const { conversationId, message, userId } = job.data;
    
    try {
      // Get LLM connection from pool
      const llm = await this.llmPool.acquire();
      
      try {
        // Process with timeout
        const response = await Promise.race([
          llm.complete(message),
          this.timeout(30000)
        ]);
        
        // Store result
        await this.storeResponse(conversationId, response);
        
        // Notify client via WebSocket
        await this.notifyClient(userId, conversationId, response);
        
        return response;
      } finally {
        // Return connection to pool
        this.llmPool.release(llm);
      }
    } catch (error) {
      // Handle failure with exponential backoff
      if (job.attemptsMade < job.opts.attempts) {
        throw error; // Will retry
      }
      
      // Final failure - notify user
      await this.notifyFailure(userId, conversationId, error);
    }
  }
}
```

## 8. Testing & Monitoring

### 8.1 Testing Strategy
```typescript
// tests/chat.test.ts
describe('Chat System', () => {
  describe('Message Handling', () => {
    it('should handle streaming responses', async () => {
      const mockStream = createMockStream([
        { type: 'token', content: 'Hello' },
        { type: 'token', content: ' world' },
        { type: 'done' }
      ]);
      
      const { result } = renderHook(() => useStreamingChat());
      
      await act(async () => {
        await result.current.sendMessage('Hi');
      });
      
      expect(result.current.streamingMessage?.content).toBe('Hello world');
    });
    
    it('should retry on network failure', async () => {
      const { result } = renderHook(() => useChatWithRetry());
      
      // Mock network failure
      server.use(
        rest.post('/api/chat', (req, res, ctx) => {
          return res.networkError('Failed to connect');
        })
      );
      
      await act(async () => {
        await result.current.sendMessage('Test');
      });
      
      expect(result.current.retryCount).toBe(1);
      expect(result.current.isRetrying).toBe(true);
    });
  });
});
```

### 8.2 Monitoring & Observability
```typescript
// monitoring/metrics.ts
class ChatMetrics {
  private prometheus = new PrometheusClient();
  
  // Key metrics to track
  messageLatency = new Histogram({
    name: 'chat_message_latency_seconds',
    help: 'Latency of chat message processing',
    labelNames: ['model', 'status'],
    buckets: [0.1, 0.5, 1, 2, 5, 10, 30]
  });
  
  streamingTokenRate = new Gauge({
    name: 'chat_streaming_tokens_per_second',
    help: 'Rate of token streaming',
    labelNames: ['conversation_id']
  });
  
  activeConversations = new Gauge({
    name: 'chat_active_conversations',
    help: 'Number of active conversations'
  });
  
  errorRate = new Counter({
    name: 'chat_errors_total',
    help: 'Total number of chat errors',
    labelNames: ['error_type']
  });
  
  // Track conversation metrics
  async trackConversation(conversationId: string, metrics: ConversationMetrics) {
    await this.saveMetrics({
      conversationId,
      messageCount: metrics.messageCount,
      totalTokens: metrics.totalTokens,
      avgResponseTime: metrics.avgResponseTime,
      userSatisfaction: metrics.userSatisfaction
    });
  }
}

// Structured logging
const logger = winston.createLogger({
  level: 'info',
  format: winston.format.json(),
  transports: [
    new winston.transports.File({ filename: 'error.log', level: 'error' }),
    new winston.transports.File({ filename: 'combined.log' }),
    new ElasticsearchTransport({
      level: 'info',
      clientOpts: { node: process.env.ELASTICSEARCH_URL }
    })
  ]
});
```

## 9. Implementation Checklist

### Backend Checklist
- [ ] **API Architecture**
  - [ ] Implement streaming endpoints with SSE/WebSocket
  - [ ] Set up message queue for async processing
  - [ ] Configure rate limiting per user/tier
  - [ ] Implement request deduplication
  - [ ] Add circuit breakers for LLM API calls

- [ ] **Data Management**
  - [ ] Design efficient database schema with proper indexes
  - [ ] Implement conversation pagination
  - [ ] Set up Redis caching layer
  - [ ] Add message compression for storage
  - [ ] Implement data retention policies

- [ ] **Security**
  - [ ] Input validation and sanitization
  - [ ] Content moderation integration
  - [ ] JWT authentication with refresh tokens
  - [ ] API key management for LLM providers
  - [ ] Audit logging for compliance

### Frontend Checklist
- [ ] **Component Architecture**
  - [ ] Virtualized message list for performance
  - [ ] Streaming message handler
  - [ ] Optimistic updates with rollback
  - [ ] Error boundary implementation
  - [ ] Accessibility features (ARIA, keyboard nav)

- [ ] **State Management**
  - [ ] React Query setup with proper cache config
  - [ ] WebSocket/SSE connection management
  - [ ] Offline support with IndexedDB
  - [ ] Message queue for offline sending
  - [ ] Conversation prefetching

- [ ] **Performance**
  - [ ] Code splitting for chat components
  - [ ] Lazy loading of message history
  - [ ] Image/file upload optimization
  - [ ] Bundle size optimization
  - [ ] Service worker for caching

### Infrastructure Checklist
- [ ] **Deployment**
  - [ ] Docker containerization
  - [ ] Kubernetes deployment configs
  - [ ] Auto-scaling policies
  - [ ] Load balancer configuration
  - [ ] CDN setup for static assets

- [ ] **Monitoring**
  - [ ] Prometheus metrics collection
  - [ ] Grafana dashboards
  - [ ] Error tracking (Sentry)
  - [ ] Log aggregation (ELK stack)
  - [ ] Uptime monitoring

- [ ] **Testing**
  - [ ] Unit tests for core logic
  - [ ] Integration tests for API
  - [ ] E2E tests for critical flows
  - [ ] Load testing setup
  - [ ] Chaos engineering tests

### Performance Targets
- Message send latency: < 200ms
- First token latency: < 1s
- Streaming rate: > 20 tokens/second
- Concurrent users: 10,000+
- Message history load: < 500ms
- Uptime: 99.9%

This architecture provides a robust, scalable foundation for an LLM chatbot that can handle production workloads while maintaining excell