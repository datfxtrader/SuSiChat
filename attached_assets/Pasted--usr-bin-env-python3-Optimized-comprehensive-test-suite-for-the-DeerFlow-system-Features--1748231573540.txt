#!/usr/bin/env python3
"""
Optimized comprehensive test suite for the DeerFlow system

Features:
- Parallel test execution
- Detailed performance benchmarking
- Resource monitoring
- Test result persistence
- CI/CD integration support
"""

import asyncio
import logging
import time
import json
import sys
import argparse
import psutil
import statistics
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Callable
from dataclasses import dataclass, asdict
from enum import Enum
from contextlib import asynccontextmanager
import aiofiles
from concurrent.futures import ProcessPoolExecutor

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Test configuration
class TestConfig:
    PARALLEL_TESTS = True
    MAX_WORKERS = 4
    LOAD_TEST_CALLS = 50
    LOAD_TEST_DURATION = 10  # seconds
    METRICS_INTERVAL = 0.1
    RESULTS_DIR = Path("test_results")
    ENABLE_PROFILING = True
    
class TestStatus(Enum):
    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"
    WARNING = "warning"

@dataclass
class TestResult:
    name: str
    status: TestStatus
    duration: float
    details: Dict[str, Any]
    error: Optional[str] = None
    warnings: List[str] = None
    metrics: Optional[Dict[str, Any]] = None
    
    def __post_init__(self):
        if self.warnings is None:
            self.warnings = []

@dataclass
class SystemMetrics:
    cpu_percent: float
    memory_mb: float
    disk_io_read: float
    disk_io_write: float
    timestamp: float

class ResourceMonitor:
    """Monitor system resources during tests"""
    
    def __init__(self):
        self.metrics: List[SystemMetrics] = []
        self.monitoring = False
        self._task = None
        
    async def start(self):
        """Start monitoring resources"""
        self.monitoring = True
        self._task = asyncio.create_task(self._monitor_loop())
        
    async def stop(self) -> Dict[str, Any]:
        """Stop monitoring and return statistics"""
        self.monitoring = False
        if self._task:
            await self._task
            
        if not self.metrics:
            return {}
            
        return {
            "cpu": {
                "avg": statistics.mean(m.cpu_percent for m in self.metrics),
                "max": max(m.cpu_percent for m in self.metrics),
                "min": min(m.cpu_percent for m in self.metrics)
            },
            "memory": {
                "avg": statistics.mean(m.memory_mb for m in self.metrics),
                "max": max(m.memory_mb for m in self.metrics),
                "min": min(m.memory_mb for m in self.metrics)
            },
            "samples": len(self.metrics)
        }
        
    async def _monitor_loop(self):
        """Continuously monitor resources"""
        process = psutil.Process()
        
        while self.monitoring:
            try:
                # Get current metrics
                cpu_percent = process.cpu_percent(interval=0.1)
                memory_info = process.memory_info()
                io_counters = process.io_counters() if hasattr(process, 'io_counters') else None
                
                metric = SystemMetrics(
                    cpu_percent=cpu_percent,
                    memory_mb=memory_info.rss / 1024 / 1024,
                    disk_io_read=io_counters.read_bytes if io_counters else 0,
                    disk_io_write=io_counters.write_bytes if io_counters else 0,
                    timestamp=time.time()
                )
                
                self.metrics.append(metric)
                await asyncio.sleep(TestConfig.METRICS_INTERVAL)
                
            except Exception as e:
                logger.error(f"Resource monitoring error: {e}")
                break

class DeerFlowTestSuite:
    """Optimized test suite for DeerFlow system"""
    
    def __init__(self):
        self.test_results: List[TestResult] = []
        self.resource_monitor = ResourceMonitor()
        self.start_time = None
        self.config = None
        self.metrics = None
        self.error_handler = None
        
    async def setup(self):
        """Initialize test dependencies"""
        try:
            from deerflow_service.config_manager import load_config, get_config
            from deerflow_service.metrics import MetricsCollector
            from deerflow_service.error_handler import error_handler
            
            self.config = load_config()
            self.metrics = MetricsCollector()
            self.error_handler = error_handler
            
            # Start resource monitoring
            if TestConfig.ENABLE_PROFILING:
                await self.resource_monitor.start()
                
            return True
        except Exception as e:
            logger.error(f"Setup failed: {e}")
            return False
    
    async def teardown(self):
        """Cleanup test resources"""
        if TestConfig.ENABLE_PROFILING:
            resource_stats = await self.resource_monitor.stop()
            logger.info(f"Resource usage: {resource_stats}")
    
    async def test_configuration_management(self) -> TestResult:
        """Test configuration loading and validation"""
        start_time = time.time()
        details = {}
        warnings = []
        
        try:
            # Test configuration loading
            config = self.config
            
            details["environment"] = config.environment
            details["config_keys"] = list(vars(config).keys())
            
            # Validate critical configurations
            critical_configs = {
                "agent.max_concurrent_tasks": config.agent.max_concurrent_tasks,
                "agent.enable_learning": config.agent.enable_learning,
                "cache.enabled": config.cache.enabled,
                "cache.ttl": config.cache.ttl
            }
            
            details["critical_configs"] = critical_configs
            
            # Check for missing configurations
            if config.agent.max_concurrent_tasks < 5:
                warnings.append("Low max_concurrent_tasks may impact performance")
                
            if not config.cache.enabled:
                warnings.append("Cache is disabled, consider enabling for better performance")
            
            # Test configuration updates
            from deerflow_service.config_manager import config_manager
            
            original_value = config.agent.max_concurrent_tasks
            config_manager.update_config({"agent": {"max_concurrent_tasks": 20}})
            updated_config = config_manager.get_config()
            
            details["update_test"] = {
                "original": original_value,
                "updated": updated_config.agent.max_concurrent_tasks,
                "success": updated_config.agent.max_concurrent_tasks == 20
            }
            
            # Restore original
            config_manager.update_config({"agent": {"max_concurrent_tasks": original_value}})
            
            status = TestStatus.PASSED if not warnings else TestStatus.WARNING
            
            return TestResult(
                name="Configuration Management",
                status=status,
                duration=time.time() - start_time,
                details=details,
                warnings=warnings
            )
            
        except Exception as e:
            return TestResult(
                name="Configuration Management",
                status=TestStatus.FAILED,
                duration=time.time() - start_time,
                details=details,
                error=str(e)
            )
    
    async def test_metrics_collection(self) -> TestResult:
        """Test metrics collection and aggregation"""
        start_time = time.time()
        details = {}
        
        try:
            metrics = self.metrics
            
            # Generate test metrics
            test_tasks = []
            for i in range(10):
                task_id = f"test_task_{i}"
                metrics.record_task_start(task_id, "test")
                
                # Simulate work
                await asyncio.sleep(0.01)
                
                # Record completion
                status = "success" if i % 3 != 0 else "failed"
                metrics.record_task_end(task_id, "test", status, 0.01)
                
                # Record tool calls
                metrics.record_tool_call(f"tool_{i % 3}", status, 0.005)
                
                # Record confidence scores
                metrics.record_confidence("test_category", 0.5 + i * 0.05)
            
            # Get metrics summary
            summary = metrics.get_metrics_summary()
            health = metrics.get_system_health()
            
            details["metrics_summary"] = summary
            details["system_health"] = health
            details["task_count"] = len(summary.get("active_tasks", {}))
            
            # Validate metrics
            assert health["overall_health"] in ["healthy", "degraded", "unhealthy"]
            assert "error_rate" in health
            assert "total_tasks" in health
            
            return TestResult(
                name="Metrics Collection",
                status=TestStatus.PASSED,
                duration=time.time() - start_time,
                details=details
            )
            
        except Exception as e:
            return TestResult(
                name="Metrics Collection",
                status=TestStatus.FAILED,
                duration=time.time() - start_time,
                details=details,
                error=str(e)
            )
    
    async def test_error_handling(self) -> TestResult:
        """Test error handling and recovery mechanisms"""
        start_time = time.time()
        details = {}
        
        try:
            from deerflow_service.error_handler import with_retry, with_circuit_breaker
            
            # Test retry mechanism
            retry_count = 0
            
            @with_retry("test_retry")
            async def flaky_function():
                nonlocal retry_count
                retry_count += 1
                if retry_count < 3:
                    raise ConnectionError(f"Attempt {retry_count} failed")
                return "Success"
            
            retry_result = await flaky_function()
            details["retry_test"] = {
                "attempts": retry_count,
                "result": retry_result,
                "success": retry_result == "Success"
            }
            
            # Test circuit breaker
            failure_count = 0
            circuit_open = False
            
            @with_circuit_breaker("test_breaker")
            async def failing_service():
                nonlocal failure_count
                failure_count += 1
                raise Exception("Service down")
            
            # Trigger circuit breaker
            for i in range(10):
                try:
                    await failing_service()
                except Exception as e:
                    if "Circuit breaker is OPEN" in str(e):
                        circuit_open = True
                        break
            
            details["circuit_breaker_test"] = {
                "failure_count": failure_count,
                "circuit_opened": circuit_open,
                "success": circuit_open
            }
            
            # Get error statistics
            error_summary = self.error_handler.get_error_summary()
            details["error_summary"] = error_summary
            
            return TestResult(
                name="Error Handling",
                status=TestStatus.PASSED,
                duration=time.time() - start_time,
                details=details
            )
            
        except Exception as e:
            return TestResult(
                name="Error Handling",
                status=TestStatus.FAILED,
                duration=time.time() - start_time,
                details=details,
                error=str(e)
            )
    
    async def test_performance_under_load(self) -> TestResult:
        """Test system performance under load"""
        start_time = time.time()
        details = {}
        
        try:
            from deerflow_service.error_handler import with_retry
            import random
            
            call_results = []
            call_times = []
            
            @with_retry("load_test")
            async def simulated_api_call(call_id: int):
                call_start = time.time()
                
                # Record start
                self.metrics.record_task_start(f"load_test_{call_id}", "api")
                
                try:
                    # Simulate variable processing time
                    await asyncio.sleep(random.uniform(0.01, 0.05))
                    
                    # Simulate failures (10% failure rate)
                    if random.random() < 0.1:
                        raise Exception("Simulated failure")
                    
                    duration = time.time() - call_start
                    self.metrics.record_task_end(f"load_test_{call_id}", "api", "success", duration)
                    
                    return {"id": call_id, "status": "success", "duration": duration}
                    
                except Exception as e:
                    duration = time.time() - call_start
                    self.metrics.record_task_end(f"load_test_{call_id}", "api", "failed", duration)
                    raise
            
            # Run load test
            load_start = time.time()
            
            # Create tasks in batches to avoid overwhelming the system
            batch_size = 10
            all_results = []
            
            for batch in range(0, TestConfig.LOAD_TEST_CALLS, batch_size):
                tasks = [
                    simulated_api_call(i) 
                    for i in range(batch, min(batch + batch_size, TestConfig.LOAD_TEST_CALLS))
                ]
                
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                all_results.extend(batch_results)
                
                # Small delay between batches
                if batch + batch_size < TestConfig.LOAD_TEST_CALLS:
                    await asyncio.sleep(0.1)
            
            load_duration = time.time() - load_start
            
            # Analyze results
            successful = [r for r in all_results if isinstance(r, dict)]
            failed = len(all_results) - len(successful)
            
            if successful:
                durations = [r["duration"] for r in successful]
                details["performance_stats"] = {
                    "total_calls": TestConfig.LOAD_TEST_CALLS,
                    "successful": len(successful),
                    "failed": failed,
                    "success_rate": len(successful) / TestConfig.LOAD_TEST_CALLS,
                    "total_duration": load_duration,
                    "throughput": TestConfig.LOAD_TEST_CALLS / load_duration,
                    "latency": {
                        "min": min(durations),
                        "max": max(durations),
                        "avg": statistics.mean(durations),
                        "p50": statistics.median(durations),
                        "p95": statistics.quantiles(durations, n=20)[18] if len(durations) > 20 else max(durations),
                        "p99": statistics.quantiles(durations, n=100)[98] if len(durations) > 100 else max(durations)
                    }
                }
            else:
                details["performance_stats"] = {"error": "All calls failed"}
            
            # Check performance thresholds
            warnings = []
            if details.get("performance_stats", {}).get("success_rate", 0) < 0.8:
                warnings.append("Success rate below 80%")
            
            if details.get("performance_stats", {}).get("latency", {}).get("p95", 1) > 0.1:
                warnings.append("P95 latency above 100ms")
            
            status = TestStatus.PASSED if not warnings else TestStatus.WARNING
            
            return TestResult(
                name="Performance Under Load",
                status=status,
                duration=time.time() - start_time,
                details=details,
                warnings=warnings
            )
            
        except Exception as e:
            return TestResult(
                name="Performance Under Load",
                status=TestStatus.FAILED,
                duration=time.time() - start_time,
                details=details,
                error=str(e)
            )
    
    async def test_system_health(self) -> TestResult:
        """Test overall system health monitoring"""
        start_time = time.time()
        details = {}
        
        try:
            # Get current system health
            health = self.metrics.get_system_health()
            error_summary = self.error_handler.get_error_summary()
            
            details["health_status"] = health
            details["error_summary"] = error_summary
            
            # Validate health metrics
            warnings = []
            
            if health["overall_health"] == "unhealthy":
                warnings.append("System health is unhealthy")
            elif health["overall_health"] == "degraded":
                warnings.append("System health is degraded")
            
            if health.get("error_rate", 0) > 0.1:
                warnings.append(f"High error rate: {health['error_rate']:.2%}")
            
            # Check resource usage if profiling is enabled
            if TestConfig.ENABLE_PROFILING and self.resource_monitor.metrics:
                current_metrics = self.resource_monitor.metrics[-1]
                details["resource_usage"] = {
                    "cpu_percent": current_metrics.cpu_percent,
                    "memory_mb": current_metrics.memory_mb
                }
                
                if current_metrics.cpu_percent > 80:
                    warnings.append(f"High CPU usage: {current_metrics.cpu_percent}%")
                
                if current_metrics.memory_mb > 500:
                    warnings.append(f"High memory usage: {current_metrics.memory_mb:.1f}MB")
            
            status = TestStatus.PASSED if not warnings else TestStatus.WARNING
            
            return TestResult(
                name="System Health",
                status=status,
                duration=time.time() - start_time,
                details=details,
                warnings=warnings
            )
            
        except Exception as e:
            return TestResult(
                name="System Health",
                status=TestStatus.FAILED,
                duration=time.time() - start_time,
                details=details,
                error=str(e)
            )
    
    async def run_tests(self, parallel: bool = True) -> bool:
        """Run all tests"""
        self.start_time = time.time()
        
        # Setup
        if not await self.setup():
            logger.error("Setup failed, aborting tests")
            return False
        
        # Define test methods
        test_methods = [
            self.test_configuration_management,
            self.test_metrics_collection,
            self.test_error_handling,
            self.test_performance_under_load,
            self.test_system_health
        ]
        
        # Run tests
        if parallel and TestConfig.PARALLEL_TESTS:
            # Run tests in parallel
            results = await asyncio.gather(
                *[test() for test in test_methods],
                return_exceptions=True
            )
            
            for result in results:
                if isinstance(result, Exception):
                    self.test_results.append(TestResult(
                        name="Unknown Test",
                        status=TestStatus.FAILED,
                        duration=0,
                        details={},
                        error=str(result)
                    ))
                else:
                    self.test_results.append(result)
        else:
            # Run tests sequentially
            for test in test_methods:
                try:
                    result = await test()
                    self.test_results.append(result)
                    
                    # Stop on critical failure
                    if result.status == TestStatus.FAILED and "Configuration" in result.name:
                        logger.error("Critical test failed, stopping execution")
                        break
                        
                except Exception as e:
                    self.test_results.append(TestResult(
                        name=test.__name__,
                        status=TestStatus.FAILED,
                        duration=0,
                        details={},
                        error=str(e)
                    ))
        
        # Teardown
        await self.teardown()
        
        # Save results
        await self.save_results()
        
        # Print summary
        self.print_summary()
        
        # Return success/failure
        failed_tests = sum(1 for r in self.test_results if r.status == TestStatus.FAILED)
        return failed_tests == 0
    
    async def save_results(self):
        """Save test results to file"""
        try:
            TestConfig.RESULTS_DIR.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = TestConfig.RESULTS_DIR / f"test_results_{timestamp}.json"
            
            results_data = {
                "timestamp": datetime.now().isoformat(),
                "duration": time.time() - self.start_time,
                "results": [asdict(r) for r in self.test_results],
                "summary": self.get_summary()
            }
            
            async with aiofiles.open(filename, 'w') as f:
                await f.write(json.dumps(results_data, indent=2, default=str))
            
            logger.info(f"Results saved to {filename}")
            
        except Exception as e:
            logger.error(f"Failed to save results: {e}")
    
    def get_summary(self) -> Dict[str, Any]:
        """Get test summary statistics"""
        total = len(self.test_results)
        passed = sum(1 for r in self.test_results if r.status == TestStatus.PASSED)
        failed = sum(1 for r in self.test_results if r.status == TestStatus.FAILED)
        warnings = sum(1 for r in self.test_results if r.status == TestStatus.WARNING)
        
        return {
            "total_tests": total,
            "passed": passed,
            "failed": failed,
            "warnings": warnings,
            "success_rate": passed / total if total > 0 else 0,
            "total_duration": time.time() - self.start_time
        }
    
    def print_summary(self):
        """Print test summary to console"""
        print("\n" + "=" * 70)
        print("🧪 DEERFLOW SYSTEM TEST RESULTS")
        print("=" * 70)
        
        summary = self.get_summary()
        
        print(f"\n📊 Summary:")
        print(f"   Total Tests: {summary['total_tests']}")
        print(f"   ✅ Passed: {summary['passed']}")
        print(f"   ❌ Failed: {summary['failed']}")
        print(f"   ⚠️  Warnings: {summary['warnings']}")
        print(f"   Success Rate: {summary['success_rate']:.1%}")
        print(f"   Duration: {summary['total_duration']:.2f}s")
        
        print(f"\n📋 Detailed Results:")
        for i, result in enumerate(self.test_results, 1):
            icon = {
                TestStatus.PASSED: "✅",
                TestStatus.FAILED: "❌",
                TestStatus.WARNING: "⚠️",
                TestStatus.SKIPPED: "⏭️"
            }.get(result.status, "❓")
            
            print(f"\n{i}. {result.name} {icon}")
            print(f"   Status: {result.status.value}")
            print(f"   Duration: {result.duration:.3f}s")
            
            if result.error:
                print(f"   Error: {result.error}")
            
            if result.warnings:
                print(f"   Warnings:")
                for warning in result.warnings:
                    print(f"     - {warning}")
            
            # Print key details
            if result.name == "Performance Under Load" and result.details.get("performance_stats"):
                stats = result.details["performance_stats"]
                print(f"   Performance:")
                print(f"     - Throughput: {stats.get('throughput', 0):.1f} calls/s")
                print(f"     - Success Rate: {stats.get('success_rate', 0):.1%}")
                print(f"     - P95 Latency: {stats.get('latency', {}).get('p95', 0)*1000:.1f}ms")

async def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description="Test the optimized DeerFlow system")
    parser.add_argument("--sequential", action="store_true", help="Run tests sequentially")
    parser.add_argument("--no-profiling", action="store_true", help="Disable resource profiling")
    parser.add_argument("--load-calls", type=int, default=TestConfig.LOAD_TEST_CALLS, 
                        help="Number of calls for load test")
    
    args = parser.parse_args()
    
    # Update configuration
    if args.no_profiling:
        TestConfig.ENABLE_PROFILING = False
    
    if args.load_calls:
        TestConfig.LOAD_TEST_CALLS = args.load_calls
    
    # Run tests
    test_suite = DeerFlowTestSuite()
    success = await test_suite.run_tests(parallel=not args.sequential)
    
    # Exit with appropriate code
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\n⚠️  Test interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n\n❌ Test suite failed: {e}")
        sys.exit(1)